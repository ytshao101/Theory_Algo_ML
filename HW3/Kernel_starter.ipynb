{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import approx_fprime\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradient(model, X, y, dimensionality, verbose=True):\n",
    "    # This checks that the gradient implementation is correct\n",
    "    w = np.random.rand(dimensionality)\n",
    "    f, g = model.funObj(w, X, y)\n",
    "\n",
    "    # Check the gradient\n",
    "    estimated_gradient = approx_fprime(w,\n",
    "                                       lambda w: model.funObj(w,X,y)[0],\n",
    "                                       epsilon=1e-6)\n",
    "\n",
    "    implemented_gradient = model.funObj(w, X, y)[1]\n",
    "\n",
    "    if np.max(np.abs(estimated_gradient - implemented_gradient) > 1e-3):\n",
    "        raise Exception('User and numerical derivatives differ:\\n%s\\n%s' %\n",
    "             (estimated_gradient[:5], implemented_gradient[:5]))\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('User and numerical derivatives agree.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotClassifier(model, X, y):\n",
    "    \"\"\"plots the decision boundary of the model and the scatterpoints\n",
    "       of the target values 'y'.\n",
    "\n",
    "    Assumptions\n",
    "    -----------\n",
    "    y : it should contain two classes: '1' and '2'\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : the trained model which has the predict function\n",
    "\n",
    "    X : the N by D feature array\n",
    "\n",
    "    y : the N element vector corresponding to the target values\n",
    "\n",
    "    \"\"\"\n",
    "    x1 = X[:, 0]\n",
    "    x2 = X[:, 1]\n",
    "\n",
    "    x1_min, x1_max = int(x1.min()) - 1, int(x1.max()) + 1\n",
    "    x2_min, x2_max = int(x2.min()) - 1, int(x2.max()) + 1\n",
    "\n",
    "    x1_line =  np.linspace(x1_min, x1_max, 200)\n",
    "    x2_line =  np.linspace(x2_min, x2_max, 200)\n",
    "\n",
    "    x1_mesh, x2_mesh = np.meshgrid(x1_line, x2_line)\n",
    "\n",
    "    mesh_data = np.c_[x1_mesh.ravel(), x2_mesh.ravel()]\n",
    "\n",
    "    y_pred = model.predict(mesh_data)\n",
    "    y_pred = np.reshape(y_pred, x1_mesh.shape)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlim([x1_mesh.min(), x1_mesh.max()])\n",
    "    plt.ylim([x2_mesh.min(), x2_mesh.max()])\n",
    "\n",
    "    plt.contourf(x1_mesh, x2_mesh, -y_pred.astype(int), # unsigned int causes problems with negative sign... o_O\n",
    "                cmap=plt.cm.RdBu, alpha=0.6)\n",
    "\n",
    "\n",
    "    y_vals = np.unique(y)\n",
    "    plt.scatter(x1[y==y_vals[0]], x2[y==y_vals[0]], color=\"b\", label=\"class %+d\" % y_vals[0])\n",
    "    plt.scatter(x1[y==y_vals[1]], x2[y==y_vals[1]], color=\"r\", label=\"class %+d\" % y_vals[1])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMin(funObj, w, maxEvals, *args, verbose=0):\n",
    "    \"\"\"\n",
    "    Uses gradient descent to optimize the objective function\n",
    "\n",
    "    This uses quadratic interpolation in its line search to\n",
    "    determine the step size alpha\n",
    "    \"\"\"\n",
    "    # Parameters of the Optimization\n",
    "    optTol = 1e-2\n",
    "    gamma = 1e-4\n",
    "\n",
    "    # Evaluate the initial function value and gradient\n",
    "    f, g = funObj(w,*args)\n",
    "    funEvals = 1\n",
    "\n",
    "    alpha = 1.\n",
    "    while True:\n",
    "        # Line-search using quadratic interpolation to \n",
    "        # find an acceptable value of alpha\n",
    "        gg = g.T.dot(g)\n",
    "\n",
    "        while True:\n",
    "            w_new = w - alpha * g\n",
    "            f_new, g_new = funObj(w_new, *args)\n",
    "\n",
    "            funEvals += 1\n",
    "            if f_new <= f - gamma * alpha*gg:\n",
    "                break\n",
    "\n",
    "            if verbose > 1:\n",
    "                print(\"f_new: %.3f - f: %.3f - Backtracking...\" % (f_new, f))\n",
    "\n",
    "            # Update step size alpha\n",
    "            alpha = (alpha**2) * gg/(2.*(f_new - f + alpha*gg))\n",
    "\n",
    "        # Print progress\n",
    "        if verbose > 0:\n",
    "            print(\"%d - loss: %.3f\" % (funEvals, f_new))\n",
    "\n",
    "        # Update step-size for next iteration\n",
    "        y = g_new - g\n",
    "        alpha = -alpha*np.dot(y.T,g) / np.dot(y.T,y)\n",
    "\n",
    "        # Safety guards\n",
    "        if np.isnan(alpha) or alpha < 1e-10 or alpha > 1e10:\n",
    "            alpha = 1.\n",
    "\n",
    "        if verbose > 1:\n",
    "            print(\"alpha: %.3f\" % (alpha))\n",
    "\n",
    "        # Update parameters/function/gradient\n",
    "        w = w_new\n",
    "        f = f_new\n",
    "        g = g_new\n",
    "\n",
    "        # Test termination conditions\n",
    "        optCond = norm(g, float('inf'))\n",
    "\n",
    "        if optCond < optTol:\n",
    "            if verbose:\n",
    "                print(\"Problem solved up to optimality tolerance %.3f\" % optTol)\n",
    "            break\n",
    "\n",
    "        if funEvals >= maxEvals:\n",
    "            if verbose:\n",
    "                print(\"Reached maximum number of function evaluations %d\" % maxEvals)\n",
    "            break\n",
    "\n",
    "    return w, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_1_plus_exp_safe(x):\n",
    "    out = np.log(1+np.exp(x))\n",
    "    out[x > 100] = x[x>100]\n",
    "    out[x < -100] = np.exp(x[x < -100])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_linear(X1, X2):\n",
    "    return X1@X2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kernelLogRegL2():\n",
    "    def __init__(self, lammy=1.0, verbose=0, maxEvals=100, \n",
    "                 kernel_fun=kernel_linear, **kernel_args):\n",
    "        self.verbose = verbose\n",
    "        self.lammy = lammy\n",
    "        self.maxEvals = maxEvals\n",
    "        self.kernel_fun = kernel_fun\n",
    "        self.kernel_args = kernel_args\n",
    "\n",
    "    def funObj(self, u, K, y):\n",
    "        yKu = y * (K@u)\n",
    "\n",
    "        # Calculate the function value\n",
    "        # f = np.sum(np.log(1. + np.exp(-yKu)))\n",
    "        f = np.sum(log_1_plus_exp_safe(-yKu))\n",
    "\n",
    "        # Add L2 regularization\n",
    "        f += 0.5 * self.lammy * u.T@K@u\n",
    "\n",
    "        # Calculate the gradient value\n",
    "        res = - y / (1. + np.exp(yKu))\n",
    "        g = (K.T@res) + self.lammy * K@u\n",
    "\n",
    "        return f, g\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n, d = X.shape\n",
    "        self.X = X\n",
    "\n",
    "        K = self.kernel_fun(X,X, **self.kernel_args)\n",
    "\n",
    "        check_gradient(self, K, y, n, verbose=self.verbose)\n",
    "        self.u, f = findMin(self.funObj, np.zeros(n), self.maxEvals, K, y, verbose=self.verbose)\n",
    "\n",
    "    def predict(self, Xtest):\n",
    "        Ktest = self.kernel_fun(Xtest, self.X, **self.kernel_args)\n",
    "        return np.sign(Ktest@self.u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_RBF(X1, X2):\n",
    "    #Your code here\n",
    "    pass\n",
    "\n",
    "def kernel_poly(X1, X2, p=2):\n",
    "    #Your code here\n",
    "    pass\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
